# Gram-ViT Training Guide

HÆ°á»›ng dáº«n train Vision Transformer vá»›i Gram + Low-Rank Residual trÃªn CIFAR-10 sá»­ dá»¥ng JAX/Flax.

---

## ğŸ“‹ Tá»•ng quan

Project nÃ y implement **Gram + Low-Rank Residual** cho MHSA output trong Vision Transformer (ViT). Correction term Ä‘Æ°á»£c thÃªm vÃ o attention branch theo cÃ´ng thá»©c:

```
Y = Z + T
T = G_t(AB)
G_t = XX^T / D (token Gram matrix)
```

vá»›i:
- A: matrix [N, r] (khá»Ÿi táº¡o Normal(0, 0.01))
- B: matrix [r, D] (khá»Ÿi táº¡o zeros - no-op ban Ä‘áº§u)
- r = 64 (rank)

---

## ğŸ› ï¸ YÃªu cáº§u há»‡ thá»‘ng

### Hardware
- **GPU**: NVIDIA GPU vá»›i CUDA support (khuyáº¿n nghá»‹: â‰¥8GB VRAM)
- **RAM**: â‰¥16GB
- **Storage**: â‰¥10GB free space (cho dataset vÃ  checkpoints)

### Software
- Python 3.8+
- CUDA 11.x
- cuDNN 8.6+

---

## ğŸ“¦ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
cd /Users/ngothanhnam/Desktop/Gram-ViT/Gram-ViT
```

### 2. Táº¡o virtual environment (khuyáº¿n nghá»‹)
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# hoáº·c
venv\Scripts\activate  # Windows
```

### 3. CÃ i Ä‘áº·t dependencies
```bash
cd vit_jax
pip install -r requirements.txt
```

**LÆ°u Ã½**: File requirements.txt bao gá»“m JAX vá»›i CUDA support. Náº¿u báº¡n chá»‰ cÃ³ CPU:
```bash
pip install jax[cpu]
```

### 4. CÃ i Ä‘áº·t Weights & Biases
```bash
pip install wandb
wandb login
```

Nháº­p API key cá»§a báº¡n khi Ä‘Æ°á»£c yÃªu cáº§u (láº¥y tá»« https://wandb.ai/authorize)

---

## ğŸš€ Cháº¡y Training

### Basic Training Command

```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_cifar10 \
  --config=vit_jax/configs/vit.py:b16,cifar10
```

### Giáº£i thÃ­ch cÃ¡c tham sá»‘:

- `--workdir`: ThÆ° má»¥c lÆ°u checkpoints, logs, vÃ  W&B data
- `--config`: Config file vá»›i format `<file>:<model>,<dataset>`
  - `vit.py`: Config file
  - `b16`: ViT-Base/16 model
  - `cifar10`: CIFAR-10 dataset

### CÃ¡c model variants kháº£ dá»¥ng:

| Model | Description | Parameters |
|-------|-------------|------------|
| `ti16` | ViT-Tiny/16 | 5.7M |
| `s16` | ViT-Small/16 | 22M |
| `b16` | ViT-Base/16 | 86M |
| `l16` | ViT-Large/16 | 307M |
| `b32` | ViT-Base/32 | 88M |

**VÃ­ dá»¥ vá»›i ViT-Small:**
```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_small \
  --config=vit_jax/configs/vit.py:s16,cifar10
```

---

## âš™ï¸ Configuration Options

### Modify Training Hyperparameters

Báº¡n cÃ³ thá»ƒ override config parameters trá»±c tiáº¿p tá»« command line:

```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_cifar10 \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.base_lr=0.001 \
  --config.total_steps=20000 \
  --config.batch=256 \
  --config.eval_every=500
```

### CÃ¡c config quan trá»ng:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `base_lr` | 0.01 (b16+cifar10) | Learning rate |
| `total_steps` | 10,000 | Tá»•ng sá»‘ training steps |
| `batch` | 512 | Batch size cho training |
| `batch_eval` | 512 | Batch size cho evaluation |
| `warmup_steps` | 500 | Warmup steps cho LR scheduler |
| `eval_every` | 100 | Evaluate sau má»—i N steps |
| `progress_every` | 10 | Log progress sau má»—i N steps |
| `checkpoint_every` | 1,000 | Save checkpoint sau má»—i N steps |

### Gram-LowRank specific configs:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `model.transformer.use_gram_lowrank_mhsa` | True | Enable Gram-lowrank residual |
| `model.transformer.gram_lowrank_rank` | 64 | Rank cho low-rank matrices |
| `model.transformer.gram_lowrank_a_init_std` | 1e-2 | Std dev cho init A matrix |

**VÃ­ dá»¥ táº¯t Gram-lowrank (baseline ViT):**
```bash
python -m vit_jax.main \
  --workdir=/tmp/baseline_vit \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.model.transformer.use_gram_lowrank_mhsa=False
```

**VÃ­ dá»¥ thay Ä‘á»•i rank:**
```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_rank128 \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.model.transformer.gram_lowrank_rank=128
```

---

## ğŸ“Š Weights & Biases Logging

### Metrics Ä‘Æ°á»£c log tá»± Ä‘á»™ng:

#### Priority 1 - Core Metrics
- **Training**: `Train/loss`, `Train/learning_rate`
- **Validation**: `Val/loss`, `Val/accuracy`, `Val/top5_accuracy`
- **Per-class accuracy**: `Val/per_class_accuracy/{class_name}` (10 classes)
- **Confusion Matrix**: `Charts/confusion_matrix`

#### Priority 2 - Optimization
- **Gradients**: `Optim/grad_global_norm`
- **Parameters**: `Optim/param_global_norm`
- **Activations**: `Activations/block_i/mhsa_out_mean`, `Activations/block_i/mhsa_out_std`
- **MLP**: `Activations/block_i/mlp_out_mean`, `Activations/block_i/mlp_out_std`

#### Gram-LowRank Specific Metrics
Cho má»—i encoder block:
- `GramLowRank/block_i/T_norm` - Norm cá»§a correction term
- `GramLowRank/block_i/Z_norm` - Norm cá»§a MHSA output
- `GramLowRank/block_i/T_over_Z_norm` - Tá»· lá»‡ T/Z (quan trá»ng!)
- `GramLowRank/block_i/A_norm` - Norm cá»§a matrix A
- `GramLowRank/block_i/B_norm` - Norm cá»§a matrix B

### Xem results trÃªn W&B:

1. Truy cáº­p: https://wandb.ai/
2. Project name: `gram-vit-cifar10`
3. Run name: `{model_name}_{dataset}`

---

## ğŸ’¾ Checkpointing

### Automatic Checkpointing

Checkpoints Ä‘Æ°á»£c tá»± Ä‘á»™ng lÆ°u má»—i `checkpoint_every` steps táº¡i `workdir/`.

### Resume Training tá»« Checkpoint

```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_cifar10 \
  --config=vit_jax/configs/vit.py:b16,cifar10
```

Training sáº½ tá»± Ä‘á»™ng resume tá»« checkpoint má»›i nháº¥t trong workdir náº¿u cÃ³.

### Checkpoint Format

```
workdir/
â”œâ”€â”€ checkpoint_1000
â”œâ”€â”€ checkpoint_2000
â”œâ”€â”€ checkpoint_3000
â””â”€â”€ ...
```

Má»—i checkpoint chá»©a:
- Model parameters
- Optimizer state
- Current step

---

## ğŸ¯ Training from Scratch vs Fine-tuning

### Training from Scratch (Recommended cho CIFAR-10)

```bash
python -m vit_jax.main \
  --workdir=/tmp/gram_vit_scratch \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.pretrained_dir='.'
```

**LÆ°u Ã½**: Code hiá»‡n táº¡i yÃªu cáº§u pretrained checkpoint. Äá»ƒ train from scratch hoÃ n toÃ n, cáº§n sá»­a code Ä‘á»ƒ skip checkpoint loading.

### Alternative: Quick Testing Config

Sá»­ dá»¥ng testing config cho debugging nhanh:
```bash
python -m vit_jax.main \
  --workdir=/tmp/test_run \
  --config=vit_jax/configs/vit.py:testing,cifar10 \
  --config.total_steps=100 \
  --config.eval_every=50
```

---

## ğŸ“ˆ Monitoring Training

### Real-time Logs

Training progress Ä‘Æ°á»£c log ra console:
```
Step: 100/10000 1.0%, img/sec/core: 234.5, ETA: 2.45h
Step: 100 Learning rate: 0.0010000, Test accuracy: 0.45123, img/sec/core: 456.7
```

### W&B Dashboard

Xem real-time metrics:
- Learning curves
- Confusion matrix
- Gradient/parameter norms
- Gram-lowrank stats (T/Z ratio)

### Check GPU Usage

```bash
nvidia-smi -l 1  # Update má»—i giÃ¢y
```

---

## ğŸ› Troubleshooting

### 1. Out of Memory (OOM)

**Giáº£m batch size:**
```bash
--config.batch=256 --config.batch_eval=256
```

**TÄƒng gradient accumulation:**
```bash
--config.accum_steps=16
```

### 2. Pretrained Checkpoint Not Found

Error: `Could not find "path/to/model.npz"`

**Solution**: Download pretrained model hoáº·c train from scratch:
```bash
# Option 1: Download tá»« Google Cloud
gsutil -m cp gs://vit_models/imagenet21k/ViT-B_16.npz /path/to/pretrained/

# Option 2: Sá»­a config Ä‘á»ƒ skip pretrained (cáº§n modify code)
```

### 3. W&B Login Issues

```bash
wandb login --relogin
```

Hoáº·c offline mode:
```bash
wandb offline
```

### 4. JAX/CUDA Version Mismatch

Reinstall JAX vá»›i Ä‘Ãºng CUDA version:
```bash
pip uninstall jax jaxlib
pip install --upgrade "jax[cuda11_cudnn86]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

### 5. Slow Training / Low GPU Utilization

- Check data pipeline bottleneck: tÄƒng `--config.prefetch=4`
- Verify XLA compilation: logs sáº½ show "Compiling..."
- Monitor with `nvidia-smi` vÃ  `htop`

---

## ğŸ“ Example Training Scripts

### Full Training Run (ViT-B/16 with Gram-lowrank)

```bash
#!/bin/bash

python -m vit_jax.main \
  --workdir=/tmp/gram_vit_full \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.total_steps=10000 \
  --config.base_lr=0.01 \
  --config.batch=512 \
  --config.eval_every=100 \
  --config.checkpoint_every=1000 \
  --config.model.transformer.use_gram_lowrank_mhsa=True \
  --config.model.transformer.gram_lowrank_rank=64
```

### Baseline ViT (No Gram-lowrank)

```bash
#!/bin/bash

python -m vit_jax.main \
  --workdir=/tmp/baseline_vit \
  --config=vit_jax/configs/vit.py:b16,cifar10 \
  --config.total_steps=10000 \
  --config.base_lr=0.01 \
  --config.batch=512 \
  --config.eval_every=100 \
  --config.model.transformer.use_gram_lowrank_mhsa=False
```

### Ablation Study: Different Ranks

```bash
#!/bin/bash

for rank in 8 16 32 64 128; do
  python -m vit_jax.main \
    --workdir=/tmp/gram_vit_rank${rank} \
    --config=vit_jax/configs/vit.py:b16,cifar10 \
    --config.model.transformer.gram_lowrank_rank=${rank}
done
```

---

## ğŸ“š Reference

### Config Files
- `vit_jax/configs/vit.py` - Main ViT configs
- `vit_jax/configs/common.py` - Common training configs
- `vit_jax/configs/models.py` - Model architecture configs

### Key Files
- `vit_jax/main.py` - Entry point
- `vit_jax/train.py` - Training loop vá»›i W&B logging
- `vit_jax/models_vit.py` - ViT architecture vá»›i Gram-lowrank
- `vit_jax/input_pipeline.py` - Data loading

### Important Papers
- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) - Original ViT paper
- [How to train your ViT?](https://arxiv.org/abs/2106.10270) - Training best practices

---

## ğŸ“ Tips for Success

1. **Start Small**: Test vá»›i `testing` config hoáº·c `s16` model trÆ°á»›c
2. **Monitor W&B**: Theo dÃµi `T_over_Z_norm` Ä‘á»ƒ xem Gram-lowrank cÃ³ hoáº¡t Ä‘á»™ng
3. **Baseline First**: Train baseline ViT Ä‘á»ƒ so sÃ¡nh
4. **Ablation Studies**: Test different ranks (8, 16, 32, 64, 128)
5. **GPU Warm-up**: First step sáº½ compile cháº­m (1-2 phÃºt), bÃ¬nh thÆ°á»ng!

---

## â“ FAQ

**Q: Training máº¥t bao lÃ¢u?**
A: ViT-B/16 trÃªn CIFAR-10, 10K steps: ~2-3 giá» trÃªn V100 GPU

**Q: LÃ m sao biáº¿t Gram-lowrank cÃ³ hiá»‡u quáº£?**
A: Xem metric `T_over_Z_norm` trÃªn W&B. Náº¿u > 0 vÃ  tÄƒng dáº§n, branch Ä‘ang há»c.

**Q: Táº¡i sao B matrix init = 0?**
A: LoRA-style initialization. Äáº£m báº£o model báº¯t Ä‘áº§u giá»‘ng baseline ViT.

**Q: CÃ³ thá»ƒ train trÃªn CPU khÃ´ng?**
A: CÃ³ nhÆ°ng ráº¥t cháº­m (>100x). KhÃ´ng khuyáº¿n nghá»‹.

**Q: Dataset Ä‘Æ°á»£c download tá»± Ä‘á»™ng?**
A: CÃ³, TensorFlow Datasets sáº½ tá»± download CIFAR-10 láº§n Ä‘áº§u cháº¡y.

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check Troubleshooting section
2. Check W&B logs Ä‘á»ƒ debug
3. Verify JAX/GPU setup: `python -c "import jax; print(jax.devices())"`

Happy Training! ğŸš€
